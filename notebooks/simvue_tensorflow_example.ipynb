{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1HDN0gUKC9pGtroOQhylrH_eM6BnIvcRC?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8naAmYqvOcG"
   },
   "source": [
    "<img src=\"https://simvue.io/images/simvue-black.png\" width=\"280\" alt=\"Simvue\" />\n",
    "\n",
    "# Simple Tensorflow example\n",
    "\n",
    "Taken from https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\n",
    "> TensorFlow 2.0 implementation of a Recurrent Neural Network (LSTM) that performs dynamic computation over sequences with variable length. This example is using a toy dataset to classify linear sequences. The generated sequences have variable length.\n",
    "\n",
    "In this example we take an existing Python code and make some minor changes in order to use Simvue to:\n",
    "\n",
    "*   Record dataset, training and network parameters as metadata,\n",
    "*   Log metrics while the code is running, in this case acurracy and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY764bW-Y3Nd"
   },
   "source": [
    "## Set an access token\n",
    "A token needs to be specified in order to authenticate to the Simvue REST API. To obtain the token, login to https://app.simvue.io/runs and click **Create new run**. Copy the token and paste it into the box when prompted and push enter.\n",
    "\n",
    "It is important to note that the token will not be saved in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwNo_ja_45Zz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"SIMVUE_URL\"] = \"https://app.simvue.io\"\n",
    "os.environ[\"SIMVUE_TOKEN\"] = getpass.getpass(prompt=\"Token: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3m7b9siaHqG"
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iixvs2NWeso"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow simvue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MKgsIcIaL4R"
   },
   "source": [
    "##Â The code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLDZTHMgv5P8"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import numpy as np\n",
    "import random\n",
    "from simvue import Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jz0hbM15wB5D"
   },
   "outputs": [],
   "source": [
    "# Dataset parameters.\n",
    "num_classes = 2  # linear sequence or not.\n",
    "seq_max_len = 20  # Maximum sequence length.\n",
    "seq_min_len = 5  # Minimum sequence length (before padding).\n",
    "masking_val = (\n",
    "    -1\n",
    ")  # -1 will represents the mask and be used to pad sequences to a common max length.\n",
    "max_value = 10000  # Maximum int value.\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 2000\n",
    "batch_size = 64\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_units = 32  # number of neurons for the LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-QpNOGY768t"
   },
   "outputs": [],
   "source": [
    "# Start a run and specify metadata\n",
    "run = Run()\n",
    "run.init(\n",
    "    metadata={\n",
    "        \"dataset.num_classes\": num_classes,\n",
    "        \"dataset.seq_max_len\": seq_max_len,\n",
    "        \"dataset.seq_min_len\": seq_min_len,\n",
    "        \"dataset.masking_val\": masking_val,\n",
    "        \"dataset.max_value\": max_value,\n",
    "        \"training.learning_rate\": learning_rate,\n",
    "        \"training.training_steps\": training_steps,\n",
    "        \"training.batch_size\": batch_size,\n",
    "        \"network.num_units\": num_units,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5o-wjSjYwFtU"
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "#  TOY DATA GENERATOR\n",
    "# ====================\n",
    "\n",
    "\n",
    "def toy_sequence_data():\n",
    "    \"\"\"Generate sequence of data with dynamic length.\n",
    "    This function generates toy samples for training:\n",
    "    - Class 0: linear sequences (i.e. [1, 2, 3, 4, ...])\n",
    "    - Class 1: random sequences (i.e. [9, 3, 10, 7,...])\n",
    "\n",
    "    NOTICE:\n",
    "    We have to pad each sequence to reach 'seq_max_len' for TensorFlow\n",
    "    consistency (we cannot feed a numpy array with inconsistent\n",
    "    dimensions). The dynamic calculation will then be perform and ignore\n",
    "    the masked value (here -1).\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Set variable sequence length.\n",
    "        seq_len = random.randint(seq_min_len, seq_max_len)\n",
    "        rand_start = random.randint(0, max_value - seq_len)\n",
    "        # Add a random or linear int sequence (50% prob).\n",
    "        if random.random() < 0.5:\n",
    "            # Generate a linear sequence.\n",
    "            seq = np.arange(start=rand_start, stop=rand_start + seq_len)\n",
    "            # Rescale values to [0., 1.].\n",
    "            seq = seq / max_value\n",
    "            # Pad sequence until the maximum length for dimension consistency.\n",
    "            # Masking value: -1.\n",
    "            seq = np.pad(\n",
    "                seq,\n",
    "                mode=\"constant\",\n",
    "                pad_width=(0, seq_max_len - seq_len),\n",
    "                constant_values=masking_val,\n",
    "            )\n",
    "            label = 0\n",
    "        else:\n",
    "            # Generate a random sequence.\n",
    "            seq = np.random.randint(max_value, size=seq_len)\n",
    "            # Rescale values to [0., 1.].\n",
    "            seq = seq / max_value\n",
    "            # Pad sequence until the maximum length for dimension consistency.\n",
    "            # Masking value: -1.\n",
    "            seq = np.pad(\n",
    "                seq,\n",
    "                mode=\"constant\",\n",
    "                pad_width=(0, seq_max_len - seq_len),\n",
    "                constant_values=masking_val,\n",
    "            )\n",
    "            label = 1\n",
    "        yield np.array(seq, dtype=np.float32), np.array(label, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGRPh018wI9n"
   },
   "outputs": [],
   "source": [
    "# Use tf.data API to shuffle and batch data.\n",
    "train_data = tf.data.Dataset.from_generator(\n",
    "    toy_sequence_data, output_types=(tf.float32, tf.float32)\n",
    ")\n",
    "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hA1660iBwLK0"
   },
   "outputs": [],
   "source": [
    "# Create LSTM Model.\n",
    "class LSTM(Model):\n",
    "    # Set layers.\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Define a Masking Layer with -1 as mask.\n",
    "        self.masking = layers.Masking(mask_value=masking_val)\n",
    "        # Define a LSTM layer to be applied over the Masking layer.\n",
    "        # Dynamic computation will automatically be performed to ignore -1 values.\n",
    "        self.lstm = layers.LSTM(units=num_units)\n",
    "        # Output fully connected layer (2 classes: linear or random seq).\n",
    "        self.out = layers.Dense(num_classes)\n",
    "\n",
    "    # Set forward pass.\n",
    "    def call(self, x, is_training=False):\n",
    "        # A RNN Layer expects a 3-dim input (batch_size, seq_len, num_features).\n",
    "        x = tf.reshape(x, shape=[-1, seq_max_len, 1])\n",
    "        # Apply Masking layer.\n",
    "        x = self.masking(x)\n",
    "        # Apply LSTM layer.\n",
    "        x = self.lstm(x)\n",
    "        # Apply output layer.\n",
    "        x = self.out(x)\n",
    "        if not is_training:\n",
    "            # tf cross entropy expect logits without softmax, so only\n",
    "            # apply softmax when not training.\n",
    "            x = tf.nn.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Build LSTM model.\n",
    "lstm_net = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhVoI5D-wNAu"
   },
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss.\n",
    "# Note that this will apply 'softmax' to the logits.\n",
    "def cross_entropy_loss(x, y):\n",
    "    # Convert labels to int 64 for tf cross-entropy function.\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    # Apply softmax to logits and compute cross-entropy.\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)\n",
    "    # Average loss across the batch.\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "\n",
    "\n",
    "# Adam optimizer.\n",
    "optimizer = tf.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kkJ-nvswTux"
   },
   "outputs": [],
   "source": [
    "# Optimization process.\n",
    "def run_optimization(x, y):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        # Forward pass.\n",
    "        pred = lstm_net(x, is_training=True)\n",
    "        # Compute loss.\n",
    "        loss = cross_entropy_loss(pred, y)\n",
    "\n",
    "    # Variables to update, i.e. trainable variables.\n",
    "    trainable_variables = lstm_net.trainable_variables\n",
    "\n",
    "    # Compute gradients.\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "\n",
    "    # Update weights following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ot5AWLKwVdH"
   },
   "outputs": [],
   "source": [
    "# Run training for the given number of steps.\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "    # Run the optimization to update W and b values.\n",
    "    run_optimization(batch_x, batch_y)\n",
    "\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        pred = lstm_net(batch_x, is_training=True)\n",
    "        loss = cross_entropy_loss(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n",
    "\n",
    "        # Log metrics to Simvue\n",
    "        run.log_metrics({\"loss\": float(loss), \"accuracy\": float(acc)}, step=step)\n",
    "\n",
    "# End the run\n",
    "run.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5keqr7gsaTPa"
   },
   "source": [
    "Once the training above has started you can login to https://app.simvue.io, go to the **Runs** page and you should be able to find the currently running training."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
